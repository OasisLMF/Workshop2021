{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Hazard Data\n",
    "\n",
    "Generate hazard module from source data\n",
    "\n",
    "In this exercise, we will extract data from Hurdat (https://www.nhc.noaa.gov/data/) and convert the data into an Oasis compliant hazard module format.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard python libraries\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "# non-standard python libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "# constants\n",
    "\n",
    "#import geopy.distance\n",
    "#km_nm_factor = 1.852\n",
    "#earth_radius = 6373.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download event data from hurdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from URL and write to file\n",
    "raw_fn = 'source_data/raw_hurdat_data.txt'\n",
    "url = 'https://www.nhc.noaa.gov/data/hurdat/hurdat2-1851-2020-052921.txt'\n",
    "data = requests.get(url)\n",
    "with open(raw_fn,'w') as fw:\n",
    "    fw.write(data.text)\n",
    "    \n",
    "# create an empty list to temoporarily store downloaded data\n",
    "data=[]\n",
    "\n",
    "# loop through downloaded data and parse by row\n",
    "with open(raw_fn,'r') as fr:\n",
    "    for row in fr:\n",
    "        if row[0:2]=='AL':\n",
    "            event_id=row[0:8]\n",
    "            event_name=row[19:28]\n",
    "            records=row[34:36]\n",
    "        else:\n",
    "            date=row[0:8]\n",
    "            time=row[10:14]\n",
    "            record_id=row[16]\n",
    "            system_status=row[19:21]\n",
    "            latitude=row[23:27]\n",
    "            lat_hem=row[27]\n",
    "            longitude=row[30:35]\n",
    "            lon_hem=row[35]\n",
    "            max_windspeed=row[39:41]\n",
    "            min_pressure=row[43:47]\n",
    "            radii_34kt_ne=row[49:53]\n",
    "            radii_34kt_se=row[55:59]\n",
    "            radii_34kt_sw=row[61:65]\n",
    "            radii_34kt_nw=row[67:71]\n",
    "            radii_50kt_ne=row[73:77]\n",
    "            radii_50kt_se=row[79:83]\n",
    "            radii_50kt_sw=row[85:89]\n",
    "            radii_50kt_nw=row[91:95]\n",
    "            radii_64kt_ne=row[97:101]\n",
    "            radii_64kt_se=row[103:107]\n",
    "            radii_64kt_sw=row[109:113]\n",
    "            radii_64kt_nw=row[115:119]\n",
    "            \n",
    "            row_data = [\n",
    "                event_id,\n",
    "                event_name,\n",
    "                records,\n",
    "                date,\n",
    "                time,\n",
    "                record_id,\n",
    "                system_status,\n",
    "                latitude,\n",
    "                lat_hem,\n",
    "                longitude,\n",
    "                lon_hem,\n",
    "                max_windspeed,\n",
    "                min_pressure,\n",
    "                radii_34kt_ne,\n",
    "                radii_34kt_se,\n",
    "                radii_34kt_sw,\n",
    "                radii_34kt_nw,\n",
    "                radii_50kt_ne,\n",
    "                radii_50kt_se,\n",
    "                radii_50kt_sw,\n",
    "                radii_50kt_nw,\n",
    "                radii_64kt_ne,\n",
    "                radii_64kt_se,\n",
    "                radii_64kt_sw,\n",
    "                radii_64kt_nw\n",
    "            ]\n",
    "            data.append(row_data)\n",
    "    \n",
    "# create a pandas dataframe with the list data\n",
    "cols = ['id','name','records','date','time','record_id','system_status','latitude',\n",
    "        'lat_hem','longitude','lon_hem','max_windspeed','min_pressure',\n",
    "        'radii_34kt_ne','radii_34kt_se','radii_34kt_sw','radii_34kt_nw',\n",
    "        'radii_50kt_ne','radii_50kt_se','radii_50kt_sw','radii_50kt_nw',\n",
    "        'radii_64kt_ne','radii_64kt_se','radii_64kt_sw','radii_64kt_nw']\n",
    "\n",
    "\n",
    "\n",
    "df_data=pd.DataFrame(data=data,columns=cols)\n",
    "\n",
    "dtypes = {'id':str,'name':str,'records':int,'date':int,'time':int,\n",
    "          'record_id':str,'system_status':str,'latitude':float,'lat_hem':str,\n",
    "          'longitude':float,'lon_hem':str,'max_windspeed':int,'min_pressure':int,\n",
    "          'radii_34kt_ne':int,'radii_34kt_se':int,'radii_34kt_sw':int,'radii_34kt_nw':int,\n",
    "          'radii_50kt_ne':int,'radii_50kt_se':int,'radii_50kt_sw':int,'radii_50kt_nw':int,\n",
    "          'radii_64kt_ne':int,'radii_64kt_se':int,'radii_64kt_sw':int,'radii_64kt_nw':int}\n",
    "\n",
    "df_data = df_data.astype(dtypes)\n",
    "\n",
    "# strip leading spaces\n",
    "df_data['name']=df_data['name'].str.strip()\n",
    "\n",
    "# set negative longitudes for western hemisphere\n",
    "df_data['longitude']=-df_data['longitude']\n",
    "\n",
    "# write the dataframe out to csv\n",
    "formatted_fn = 'source_data/formatted_hurdat_data.csv'\n",
    "df_data.to_csv(formatted_fn,index=False)\n",
    "\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the data for Hurricane HARVEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have access to the hurdat website, the data can be found here\n",
    "# df_data = pd.read_csv('source_data/formatted_hurdat_data.csv')\n",
    "\n",
    "df_data[df_data['id']=='AL092017'].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get model area grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get events which make happened since 2000\n",
    "df_data_in_period = df_data[df_data['date']>=20000101]\n",
    "\n",
    "df_data_in_period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a list of distinct events in the remaining event set\n",
    "\n",
    "this is used in Oasis to batch the exection job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data\n",
    "df_events = df_data_in_period['id'].drop_duplicates().reset_index()[['id']]\n",
    "df_events['event_id']=df_events.index+1\n",
    "\n",
    "df_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the event ids into the data dataframe\n",
    "df_data_in_period = df_data_in_period.merge(df_events,on='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a uniform grid for the US\n",
    "\n",
    "This is used to discretise the hazard data over area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid = pd.read_csv('source_data/us_grid.csv')\n",
    "df_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display the grid on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show grid\n",
    "geometry = [Point(xy) for xy in zip(df_grid['longitude'], df_grid['latitude'])]\n",
    "gdf = GeoDataFrame(df_grid, geometry=geometry)   \n",
    "\n",
    "#this is a simple map that comes with geopandas\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf.plot(ax=world.plot(figsize=(20, 12)), marker='o', color='red', markersize=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the cartesian indicies (x,y) for the grid and add in area peril id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cartisian indicies\n",
    "df_x = df_grid[['latitude']].drop_duplicates().reset_index()[['latitude']]\n",
    "df_x['x_index']=df_x.index+1\n",
    "\n",
    "df_y = df_grid[['longitude']].drop_duplicates().reset_index()[['longitude']]\n",
    "df_y['y_index']=df_y.index+1\n",
    "\n",
    "df_grid_coord = df_grid.merge(df_x,on='latitude').merge(df_y,on='longitude')\n",
    "\n",
    "# create area peril id for grid\n",
    "df_grid_coord['area_peril_id']=df_grid_coord.index+1\n",
    "\n",
    "del df_grid_coord['geometry']\n",
    "\n",
    "df_grid_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find areaperil cell of track point\n",
    "df_events_ap = df_data_in_period.merge(df_grid_coord,on=['latitude','longitude'])\n",
    "\n",
    "df_events_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove hazard records below a windspeed threshold\n",
    "\n",
    "we don't need low windspeed records, as they don't cause damage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove records below windspeed threhold\n",
    "v_thresh = 45\n",
    "df_events_thresh = df_events_ap[df_events_ap['max_windspeed'] >= v_thresh]\n",
    "\n",
    "df_events_thresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display remaining events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_thresh[['id','name']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### index the intensity values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index windspeeds and assign size\n",
    "df_intensity = df_events_thresh['max_windspeed'].drop_duplicates().sort_values().reset_index()\n",
    "df_intensity['intensity_bin_index']=df_intensity.index+1\n",
    "\n",
    "df_intensity = df_intensity[['intensity_bin_index','max_windspeed']]\n",
    "\n",
    "df_intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_intensity = df_events_thresh.merge(df_intensity,on='max_windspeed').sort_values(by=['date','time'])\n",
    "df_events_intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map the area peril ids to the footprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ap(x,y):\n",
    "    print(x,y)\n",
    "    area_peril_id = df_grid_coord[(df_grid_coord['x_index']==x) &\n",
    "                    (df_grid_coord['y_index']==y)]['area_peril_id'].values[0]\n",
    "    \n",
    "    return area_peril_id\n",
    "\n",
    "x=55\n",
    "y=197\n",
    "\n",
    "try:\n",
    "    ap = df_grid_coord[(df_grid_coord['x_index']==x) &\n",
    "                    (df_grid_coord['y_index']==y)]['area_peril_id'].values[0]\n",
    "    print(ap)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_events_intensity = df_events_thresh.merge(df_intensity,on='max_windspeed').sort_values(by=['date','time'])\n",
    "\n",
    "# generate intensity values per event & areaperil\n",
    "\n",
    "\n",
    "lst_fp = []\n",
    "\n",
    "def get_ap(x,y):\n",
    "    try:\n",
    "        area_peril_id = df_grid_coord[(df_grid_coord['x_index']==x) &\n",
    "                    (df_grid_coord['y_index']==y)]['area_peril_id'].values[0]\n",
    "    except:\n",
    "        area_peril_id = 0\n",
    "    return area_peril_id\n",
    "    \n",
    "for index, row in df_events_intensity.iterrows():\n",
    "    event_id = row['event_id']\n",
    "    area_peril_id = row['area_peril_id']\n",
    "    intensity_bin = row['intensity_bin_index']\n",
    "        \n",
    "    row_fp = [event_id,area_peril_id,intensity_bin]\n",
    "    lst_fp.append(row_fp)\n",
    "    \n",
    "    # assume \"radius\" = intensity bin\n",
    "    radius = intensity_bin\n",
    "    if radius > 1:\n",
    "        x0 = df_grid_coord[df_grid_coord['area_peril_id']==area_peril_id]['x_index'].values[0]\n",
    "        y0 = df_grid_coord[df_grid_coord['area_peril_id']==area_peril_id]['y_index'].values[0]\n",
    "        for r in range(radius-1):\n",
    "            intensity_bin = intensity_bin - 1\n",
    "            x0 = x0-1\n",
    "            y0 = y0+1\n",
    "            area_peril_id = get_ap(x0,y0)\n",
    "            row_fp = [event_id,area_peril_id,intensity_bin]\n",
    "            lst_fp.append(row_fp)\n",
    "            for c in range(2*r-1):\n",
    "                x0=x0+1\n",
    "                area_peril_id = get_ap(x0,y0)\n",
    "                if area_peril_id > 0:\n",
    "                    row_fp = [event_id,area_peril_id,intensity_bin]\n",
    "                    lst_fp.append(row_fp)\n",
    "            for c in range(2*r-1):\n",
    "                y0=y0-1\n",
    "                area_peril_id = get_ap(x0,y0)\n",
    "                if area_peril_id > 0:\n",
    "                    row_fp = [event_id,area_peril_id,intensity_bin]\n",
    "                    lst_fp.append(row_fp)\n",
    "            for c in range(2*r-1):\n",
    "                x0=x0-1\n",
    "                area_peril_id = get_ap(x0,y0)\n",
    "                if area_peril_id > 0:\n",
    "                    row_fp = [event_id,area_peril_id,intensity_bin]\n",
    "                    lst_fp.append(row_fp)\n",
    "            for c in range(2*r-1-1):\n",
    "                y0=y0+1\n",
    "                area_peril_id = get_ap(x0,y0)\n",
    "                if area_peril_id > 0:\n",
    "                    row_fp = [event_id,area_peril_id,intensity_bin]\n",
    "                    lst_fp.append(row_fp)                \n",
    "    \n",
    "df_footprint = pd.DataFrame(data=lst_fp,columns=['event_id','area_peril_id','intensity_bin_index'],dtype='int').sort_values(\n",
    "    by=['event_id','area_peril_id','intensity_bin_index'])\n",
    "df_footprint['probability']=1\n",
    "\n",
    "df_footprint\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write model files out\n",
    "df_events[['event_id','id']].to_csv('model_data/events.csv',index=False)\n",
    "df_footprint[df_footprint['area_peril_id']>0].to_csv('model_data/footprint.csv',index=False)\n",
    "df_intensity.to_csv('model_data/intensity_bin_dict.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the area peril grid for use in mapping exposure (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write keys data out\n",
    "df_grid_coord[['area_peril_id','latitude','longitude']].to_csv('keys_data/areaperil_dict.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate occurrence file\n",
    "\n",
    "this is used in oasis to map events to years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write occurrence file\n",
    "df_event_dates = df_events_intensity.groupby(['id'])[['date']].min().merge(df_events,on='id')\n",
    "df_event_dates['date_str']=df_event_dates['date'].astype('str')\n",
    "df_event_dates['occ_year']=df_event_dates['date_str'].apply(lambda x: int(x[0:4]))\n",
    "df_event_dates['occ_month']=df_event_dates['date_str'].apply(lambda x: int(x[4:6]))\n",
    "df_event_dates['occ_day']=df_event_dates['date_str'].apply(lambda x: int(x[6:8]))\n",
    "\n",
    "year_min = df_event_dates['occ_year'].min()\n",
    "df_event_dates['period_no']=df_event_dates['occ_year'] - year_min + 1\n",
    "\n",
    "df_occurrence = df_event_dates[['event_id','period_no','occ_year','occ_month','occ_day']].sort_values(\n",
    "    by=['period_no','event_id'])\n",
    "\n",
    "df_occurrence.to_csv('model_data/occurrence.csv',index=False)\n",
    "\n",
    "df_occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate a return periods file\n",
    "\n",
    "this defines the return periods which we want to report on in the output EP curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write return period file\n",
    "with open('model_data/returnperiods.csv','w') as rp:\n",
    "    rp.write('20\\n10\\n5\\n3\\n2\\n1')\n",
    "             \n",
    "! cat model_data/returnperiods.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
